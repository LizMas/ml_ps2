---
title: "Problem Set 2"
author: "Liz Masten"
date: "10/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AER)
library(fastDummies)
library(glmnet)
library(class)
library(tidyverse)

```

# Conceptual Questions 

### 1. The table below contains a training dataset of 6 observations, 3 predictors and 1 qualitative outcome variable. Suppose we wish to use this data set to make a prediction for Y when X1= X2 = X3 = 1 using K-nearest neighbors. (2 pts)

(a) Compute the Euclidean distance between observation 3 and the test point, X1 = X2 =X3 = 1

Because each X value for Observation 3 is 0, the Euclidean distance between Observation 3 and the test points of 1 is 1. 

(b) Using Euclidean distance, what is our prediction for an observation with X1= X2 = X3 = 1 with K = 1? Why?

Our Euclidean distance is 1, so our prediction is black because Observations 4 and 6 each have 2/3 outcome as 1. 

(c) Using Euclidean distance, what is our prediction with K = 3? Why?

Now that K=3, 2 observations are still black but the third observation is white because Observations 5 and 3 are both plus or minus 1 from 1. 

(d) Suppose the table looked like this instead (i.e. a regression problem)

### 2. Let’s say I have a model with 30 potential covariates. How many potential variants on the
models can I have? What does this imply about the tradeoff between forward/backward
selection over best subset selection. (0.5 pts)



### 3. If the underlying data is highly linear, we would expect QDA to outperform LDA. True or
False? (0.5 pts)

False- LDA outperforms QDA when data is linear because LDA is Linear Discriminant Analysis. 

### 4. We have a dataset of genetics sequencing outcome, with 30 observations and 4000 variables. You are trying to determine the best method for regression analysis. Colleague A is advocating for KNN, Colleague B is advocating for linear regression, Colleague C thinks Colleague A and B are both wrong. Who should you side with? (1 pt)

Parametric models generally outperform non-parametric models on datasets with a small number of observations per feature. Because KNN is non-parametric, this is not a good choice. However, linear regression does not perform well when the number of features exceeds the number of observations. Therefore, Colleague C is correct- neither KNN nor linear regression would be good for this dataset. 

### 5. We have a dataset of genetics sequencing outcome, with 3,000 observations and 40,000 variables. You are trying to determine the best method for classification analysis. Colleague A is advocating for QDA but Colleague C is worried. Why might she be concerned? (1 pt)

Again, we have more variables than observations, making this dataset very wide. QDA requires estimation of more parameters, which is not good when the dataset already has more variables than observations. This will likely lead to overfitting and is a poor choice in this scenario. 


### 6. You have a dataset that is all dummy variables (i.e. 0/1 categorical variables). If you want to use a linear decision boundary, would you expect LDA or a logistic regression to perform better? (0.5 pts)

Logistic regression will perform better because logistic regressions are bounded between 0 and 1. 


### 7. What is the shrinkage penalty for ridge regression? (0.5pts)

Ridge regression uses L2 for shrinkage penalty, which means that it uses the square of coefficients. 

### 8. What is the shrinkage penalty for lasso regression? (0.5pts)

The shrinkage penalty for Lasso uses L1, meaning that it takes the absolute value of coefficients. 


### 9. How do the different shrinkage penalties influence variable selection for lasso vs ridge? (0.5pts)

Lasso regressions can perform variable selection by making some variables 0, meaning they can be excluded from the model. Ridge regressions can drive some coefficients close to zero, but they will never be zero. Thus, Ridge does not perform variable selection (it would then be up to the researcher to decide how to proceed with the coefficients post-Ridge regression).  

# Data Questions 

### 1. How many observations have missing values for at least one feature? Drop those observations
for now. (1 pt) 

One observation contained at least one NA value. 

```{r}

data("Fatalities")

nrow(Fatalities)

``` 

```{r}

data <- Fatalities %>% 
  drop_na()


nrow(data)

```


### 2. Which variables are categorical variables? How many classes do each of these categorical variables have? (1 pt)

5 variables are categorical: 

state (Factor)
year (Factor)
breath (Factor)
jail (Factor)
service (Factor)


```{r}

str(data)

```

Now, consider the prediction problem where you want to predict number of single vehicle fatalities (Fatalities) given all other variables available in the data set.

### 3. Convert the categorical variables to indicator variables (also called “dummy” variables) and run a linear regression. What is the adjusted R2? (1 pt)

Adjusted R-squared:  0.981 

```{r}

# making all factors binary, even year and state, which is kind of a new idea to me. 

# move column 19 (sfatal) to last column 

binary <- data %>% 
          dummy_cols(select_columns = c("state", "breath", "jail", "service", "year"),
                     remove_selected_columns = TRUE, remove_first_dummy = TRUE) %>%        
          select(-fatal, -nfatal, -fatal1517, -nfatal1517, -fatal1820, -nfatal1820, -fatal2124, -nfatal2124, -afatal) %>%
          select_(.dots = c(setdiff(names(.), 'sfatal'), 'sfatal'))
       

model_3 <- lm(sfatal ~ ., data = binary)

#This is where the R Squared is, but the print-out is huge, so I won't show it. 

find_r2 <- summary(model_3)

```

### 4. Run lasso regression with cross-validation using the canned function cv.glmnet from the package glmnet. You can use the λ sequence generated by grid function we used in section notes 4. In order to receive credit for this question, make the line immediately preceding this command say set.seed(222) and run the two lines together. Please report all numbers by rounding to three decimal places. (2 pts)

```{r}

whereami <- colnames(binary)

# sfatal is now column 76

# will this get rid of a weird error msg? 

binary$breath <- as.numeric(binary$breath)
binary$jail <- as.numeric(binary$jail)
binary$service <- as.numeric(binary$service)



set.seed(222)


lasso <- cv.glmnet(x = as.matrix(binary[,1:75]),
                   y = as.numeric(binary[,76]), 
                   nfold = 5, 
                   standardize = TRUE)  

print(lasso$lambda)

# print(round(lasso$cvm,3))

```


• Which λ had the lowest mean cross-validation error for 5 fold cross validation?

Lambda number 76 

```{r}

# print(lasso$lambda.min) %>% round(digits = 3)

```

• What was the cross-validation error?

```{r}

print(lasso$lambda.min) %>% round(digits = 3)

```


• What was the standard error of the mean cross-validation error for this value of λ?

```{r}

#?

```


• What was the largest value of λ whose mean cross validation error was within one standard deviation of the lowest cross-validation error?

```{r}

#lambda.1se tells us thelambda w/in 1 SE of min lambda

print(lasso$lambda.1se) %>% round(digits = 3)

```



### 5. Using the same data, implement your own 5-fold cross-validation routine for KNN for k =
1, ..., 20 (e.g. write the cross-validation routine yourself rather than using a canned package).
In the 90s, a popular policy response to high rates of alcohol related fatalities was to increase
taxes on alcohol. Consider the prediction problem of predicting beer tax (tax on cases of
beer) using all of the other variables. Include the snippet of code you wrote here. It should
not exceed 20 lines. Which k is best according to CV? (2 pts)

```{r}

cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
    
    
    fold_ids      <- rep(seq(kfolds), 
                         ceiling(nrow(data_x) / kfolds))
                         
    fold_ids      <- fold_ids[1:nrow(data_x)]
    
    fold_ids      <- sample(fold_ids, length(fold_ids))
    
    CV_error_mtx  <- matrix(0, 
                            nrow = length(k_seq), 
                            ncol = kfolds)
    
    for (k in k_seq) {
      for (fold in 1:kfolds) {

        knn_fold_model    <- knn(train = data_x[which(fold_ids != fold),],
                                 test = data_x[which(fold_ids == fold),],
                                 cl = data_y[which(fold_ids != fold)],
                                 k = k)
        
        CV_error_mtx[k,fold]  <- mean(knn_fold_model !=
                                        data_y[which(fold_ids == fold)])
      }
    }
    
    return(CV_error_mtx)
}

set.seed(222) 

 knn_cv_error  <- cross_validation_KNN(data_x = binary[,-76],
                                                data_y = binary[,76],
                                                k_seq = seq(20),
                                                kfolds = 5)
print(knn_cv_error)

mean_cv_error <- rowMeans(knn_cv_error)

x <- c(1:20)

df <- as.data.frame(mean_cv_error) 

df <- tibble::rowid_to_column(df, "k")

``` 

### 6. Plot mean cross-validation MSE as a function of k. Label the y-axis “Mean CV MSE” and
the x-axis “k”. (1 pt)

```{r}

plot <- ggplot(df, 
               aes(x =k,
                   y = mean_cv_error)) +
        geom_line() +
        labs(title = "Mean CV MSE as a function of k", 
             x = "k",
             y = "Mean CV MSE") 
plot

```


























